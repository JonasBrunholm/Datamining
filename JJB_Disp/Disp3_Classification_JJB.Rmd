---
title: "JJB_Disp_3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressMessages(library(MASS))
suppressMessages(library(tidyverse))
suppressMessages(library(naivebayes))
```

Used for labeled data, where the groups have to relation to each other, in the sense that $A<B$ has no meaning.

Parametric Classification is when we assume some model/distribution in order to
classify, compared to kNN and k-means when we assume no model/distribution, i.e.
they are non-parametric classifaction methods.

* Logit

* **Naive Bayes**

* **Linear Discriminant Analysis (LDA)**

* **Quadratic Discriminant Analysis (QDA)**

### (Naive) Bayes

(Naive) Bayes Classifier is almost like the LDA and QDA. It uses Bayes' theorem
like previously;

\begin{align*}
P(y=k|X) = \frac{P(y=k)P(X|y=k)}{P(X)}
\end{align*}

Then the Bayes classifier disregards the denominator, because it is the same
for all classes. Hence Bayes Classifier assigns an object to the class that it is
most likely to be part of, that is

\begin{align*}
\hat{y} = \arg\ \underset{k}{\max}\ P(y=k)P(X|y=k)
\end{align*}

The (Naive) Bayes then as previously estimates $P(y=k)$ as the proportion of the
amount of objects in class $k$, just like in LDA and QDA. Recall that $X$ is a $p-$dimensional vector, thus $P(X|y=k)$ comes from a multivariate distribution. 
Estimating this multivariate distribution is computationally heavy, which implies
that the method is unstable due to the curse of dimensionality. If we further assume 
that the components of $X$ are independent within each class, we get the Naive
Bayes classifier. This is much simpler to estimate. Hence by this assumption, we
can write

\begin{align*}
P(X|y=k) = \prod_{j=1}^PP(X_j|y = k)
\end{align*}

which implies that all the univariate marginal distribution can be estimated 
separately. Typical choice of the univarite distrubtions are Normal, so we need
to estimate 

\begin{align*}
X_j|(y=k) \sim N(\mu_{j,k},\sigma^2_{j,k})
\end{align*}

That is, compute the mean and variance for each univariate distribution. We once
again estimate 

\begin{align*}
\hat{\mu}_{j,k} = \frac{1}{n_k}\sum_{x_i|y_i = k}x_{j,k}
\end{align*}

and

\begin{align*}
\hat{\sigma}_{j,k}^2 = \frac{1}{n_k}\sum_{x_i|y_i = k}(x_{j,k}-\hat{\mu}_{j,k})^2
\end{align*}

### LDA

When one uses logistic regression, the goal is to model $P(y=j|X)$ whereas one can apply Bayes formula, to obation

\begin{align*}
P(y=k|X)=\frac{P(y=k)P(X|y=k)}{P(X)}=\frac{\pi_k f_k(x)}{\sum_{j=1}^Kf_j(x)\pi_j},
\end{align*}

using LTP on $P(X)$, where $y\in \{1,\ldots,K\}$, $\pi_k=P(y=k)=\frac{n_k}{n}$ and $f_k(x)=P(X|y=k)$.

From the equation we can see that to classify a new observation, $X_0$ we need to calculated the highest value of $P(y=k|X_0)$ for $k=1,\ldots,K$.

Thus we need $f_k(x)$, which is where we make the important assumptions.

1. Multivariate normally distributed

\begin{align}
f_i(x)=\frac{1}{(2\pi)^{p/2}|\Sigma_i|^{1/2}}exp\left(-\frac{1}{2}(x-\mu_i)^T\Sigma^{-1}_i(x-\mu_i)\right)
\end{align}

2. Assume that $\Sigma_i=\Sigma$ for $i=1,\ldots,K$.

Using the two assumptions we take the log:

\begin{align}
log(P(y=k|X))&= log(\pi_k) + log(f_k(x)) - log(\sum_{j=1}^Kf_j(x)\pi_j) \\
& \propto log(\pi_k) + log(f_k(x))\\
&=log(\pi_k) - log\left((2\pi)^{p/2}|\Sigma|^{1/2}\right) -\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)\\
\end{align}

Since we want to compare between classes we can omit $log\left((2\pi)^{p/2}|\Sigma|^{1/2}\right)$, furthermore we have

\begin{align}
(x-\mu_i)^T\Sigma^{-1}_i(x-\mu_i)&= x^T\Sigma^{-1}x-x^T\Sigma^{-1}\mu_i-\mu_i^T\Sigma^{-1}x+\mu_i^T\Sigma^{-1}\mu_i\\
&= x^T\Sigma^{-1}x-2x^T\Sigma^{-1}\mu_i+\mu_i^T\Sigma^{-1}\mu_i
\end{align}

Using the same argument as before, and the equation above we get

\begin{align}
\delta_k(x)=log(\pi_k) + x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k
\end{align}

Thus the class that maximizes the expression, is the same class that maximizes $P(y=k|X)$.

Lastly we have 
\begin{align}
\hat{\mu}=\frac{1}{n_k}\sum_{i=1}^{n_k}x_i, \quad \hat{\Sigma}=\frac{1}{n-K}\sum_{k=1}^K\sum_{i=1}^{n_k}(x_i-\hat{\mu}_k)(x_i-\hat{\mu}_k)^T
\end{align}

### QDA

Same groundwork, however we relax the assumtions on $\Sigma_k$ thus the $\delta_k(x)$ is no longer a linear classifier, and is given by

\begin{align}
\delta_k(x)=log(\pi_k)-\frac{1}{2}x^T\Sigma_k^{-1}x+x^T\Sigma_k^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma_k\mu_k
\end{align}

where 

\begin{align}
 \hat{\Sigma}_k=\frac{1}{n_k-1}\sum_{i=1}^{n_k}(x_i-\hat{\mu}_k)(x_i-\hat{\mu}_k)^T
\end{align}


### Considerations

Assumption of independence seems harsh. Tends to work anyway. Great for avoiding
the curse of dimensionality. Less observations needed to estimate univariate 
distributions than for a multivariate. If the assumption is not true, risk of bias.
Kind of like the bias/variance trade-off. Depending on data, other distributions
can be used, not only the normal distribution, e.g. $t$ distribution, log-normal.

## Practical application

In this part I am going to use LDA, QDA and Naive Bayes on the iris data set,
by only using the two first columns (Sepal Length and Sepal Width)

```{r}

data("iris") 
Data <- iris %>% dplyr::select(Sepal.Length, Sepal.Width, Species)
Data$Species <- as.numeric(Data$Species) #Selecting the Categories and the two first
#columns
head(Data) # A look at the data
par(mfrow=c(1,1))
plot((Data %>% dplyr::select(-Species)), pch = Data$Species-1, 
     col = Data$Species+1)
#LDA fit

lda.fit <- lda(Species ~ ., data = Data)

#QDA fit

qda.fit <- qda(Species ~ ., data = Data)

#Naive Bayes

bayes_fit <- naive_bayes(as.factor(Species) ~ ., data = Data)

#Making a grid to present the results;

xmin <- min(Data$Sepal.Length)-1
xmax <- max(Data$Sepal.Length)+1
ymin <- min(Data$Sepal.Width)-1
ymax <- max(Data$Sepal.Width)+1

#Generating plots

Points <- 100

x_grid = seq(xmin, xmax, length.out = Points)
y_grid <- seq(ymin, ymax, length.out = Points)

par(mfrow=c(1,3))
par(mar= c(2,2,2,2))
#The plot
plot((Data %>% dplyr::select(-Species)), pch = Data$Species-1, 
     col = Data$Species+1, main = "LDA")

#Predictions for LDA

for (j in 1:Points) {
  for (k in 1:Points) {
    new <- data.frame(Sepal.Length = x_grid[j], Sepal.Width = y_grid[k])
    new.pred <- predict(lda.fit, newdata = new)
    curr_obs <- as.numeric(new.pred$class)
    points(x_grid[j], y_grid[k], col = curr_obs+1, pch = 3, cex = 0.5)
  }
}
#Plot

plot((Data %>% dplyr::select(-Species)), pch = Data$Species-1, 
     col = Data$Species+1, main = "QDA")

#Predictions for QDA

for (j in 1:Points) {
  for (k in 1:Points) {
    new <- data.frame(Sepal.Length = x_grid[j], Sepal.Width = y_grid[k])
    new.pred <- predict(qda.fit, newdata = new)
    curr_obs <- as.numeric(new.pred$class)
    points(x_grid[j], y_grid[k], col = curr_obs+1, pch = 3, cex = 0.5)
  }
}

#Plot

plot((Data %>% dplyr::select(-Species)), pch = Data$Species-1, 
     col = Data$Species+1, main = "Naive Bayes")

for (j in 1:Points) {
  for (k in 1:Points) {
    new <- data.frame(Sepal.Length = x_grid[j], Sepal.Width = y_grid[k])
    new.pred <- predict(bayes_fit, newdata = new)
    curr_obs <- as.numeric(new.pred)
    points(x_grid[j], y_grid[k], col = curr_obs+1, pch = 3, cex = 0.5)
  }
}

```
