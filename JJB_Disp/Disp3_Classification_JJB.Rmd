---
title: "JJB_Disp_3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

Used for labeled data, where the groups have to relation to each other, in the sense that $A<B$ has no meaning.

* Logit

* **Naive Bayes**

* **Linear Discriminant Analysis (LDA)**

* **Quadratic Discriminant Analysis (QDA)**

### Naive Bayes


### LDA

When one uses logistic regression, the goal is to model $P(y=j|X)$ whereas one can apply Bayes formula, to obation

\begin{align*}
P(y=k|X)=\frac{P(y=k)P(X|y=k)}{P(X)}=\frac{\pi_k f_k(x)}{\sum_{j=1}^Kf_j(x)\pi_j},
\end{align*}

using LTP on $P(X)$, where $y\in \{1,\ldots,K\}$, $\pi_k=P(y=k)=\frac{n_k}{n}$ and $f_k(x)=P(X|y=k)$.

From the equation we can see that to classify a new observation, $X_0$ we need to calculated the highest value of $P(y=k|X_0)$ for $k=1,\ldots,K$.

Thus we need $f_k(x)$, which is where we make the important assumptions.

1. Multivariate normally distributed

\begin{align}
f_i(x)=\frac{1}{(2\pi)^{p/2}|\Sigma_i|^{1/2}}exp\left(-\frac{1}{2}(x-\mu_i)^T\Sigma^{-1}_i(x-\mu_i)\right)
\end{align}

2. Assume that $\Sigma_i=\Sigma$ for $i=1,\ldots,K$.

Using the two assumptions we take the log:

\begin{align}
log(P(y=k|X))&= log(\pi_k) + log(f_k(x)) - log(\sum_{j=1}^Kf_j(x)\pi_j) \\
& \propto log(\pi_k) + log(f_k(x))\\
&=log(\pi_k) - log\left((2\pi)^{p/2}|\Sigma|^{1/2}\right) -\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)\\
\end{align}

Since we want to compare between classes we can omit $log\left((2\pi)^{p/2}|\Sigma|^{1/2}\right)$, furthermore we have

\begin{align}
(x-\mu_i)^T\Sigma^{-1}_i(x-\mu_i)&= x^T\Sigma^{-1}x-x^T\Sigma^{-1}\mu_i-\mu_i^T\Sigma^{-1}x+\mu_i^T\Sigma^{-1}\mu_i\\
&= x^T\Sigma^{-1}x-2x^T\Sigma^{-1}\mu_i+\mu_i^T\Sigma^{-1}\mu_i
\end{align}

Using the same argument as before, and the equation above we get

\begin{align}
\delta_k(x)=log(\pi_k) + x^T\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k
\end{align}

Thus the class that maximizes the expression, is the same class that maximizes $P(y=k|X)$.

Lastly we have 
\begin{align}
\hat{\mu}=\frac{1}{n_k}\sum_{i=1}^{n_k}x_i, \quad \hat{\Sigma}=\frac{1}{n-K}\sum_{k=1}^K\sum_{i=1}^{n_k}(x_i-\hat{\mu}_k)(x_i-\hat{\mu}_k)^T
\end{align}

### QDA

Same groundwork, however we relax the assumtions on $\Sigma_k$ thus the $\delta_k(x)$ is no longer a linear classifier, and is given by

\begin{align}
\delta_k(x)=log(\pi_k)-\frac{1}{2}x^T\Sigma_k^{-1}x+x^T\Sigma_k^{-1}\mu_k-\frac{1}{2}\mu_k^T\Sigma_k\mu_k
\end{align}

where 

\begin{align}
 \hat{\Sigma}_k=\frac{1}{n_k-1}\sum_{i=1}^{n_k}(x_i-\hat{\mu}_k)(x_i-\hat{\mu}_k)^T
\end{align}


