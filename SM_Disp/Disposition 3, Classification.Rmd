---
title: 'Topic 3; Classification: LDA, QDA, Naive Bayes'
author: "Jonas Mikkelsen"
date: "29/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Parametric Classification

Parametric Classification is when we assume some model/distribution in order to
classify, compared to kNN and k-means when we assume no model/distribution, i.e.
they are non-parametric classifaction methods. 

Typically, Logistic regression is used when only two classes are present. However,
it is possible to use it for more. There are two approaches for this. 

The first is known as one vs one, where you for e.g. three classes first decide
whether each observation belongs to class 0 or 1, then 1 or 2, and finally 0 or
2. The class it was put in the most is then the class we decide in the end. 

The second approach is one vs all, where you test whether each observation belongs
to a certain class or the remaining ones. Then decide by majority rule as before.

## Linear Discriminant Analysis (LDA)

It is similar to logistic regression. Logistic regression models $P(y=j|X)$, but
LDA instead uses Bayes' theorem

\begin{align*}
P(y=k|X) = \frac{P(y=k)P(X|y=k)}{P(X)} = \frac{\pi_k f_k(x)}{\sum_{i=1}^K\pi_if_i(x)}
\end{align*}

where $y\in\{1,\ldots, K\}, \pi_k = P(y=k),f_k(x)=P(X|y=k)$. For the denominator, 
note that $P(X) = \sum_{i=1}^kP(X|y = i)$ which is why the denominator has that 
form. To estimate $\pi_k=P(y=k)$, we let it equal to the proportion of observations
that belongs to class $k$. 

For the remaining part, we only need to get $f_k(x)=P(X|y=k)$. To do this, we must
assume some distribution in order to get it. For LDA, we assume that $f_k(x)$ is
multivariate normal, which means that 

\begin{align*}
f_k(x)=\frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2}(
x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)\right).
\end{align*}

LDA also assumes that $\Sigma_k=\Sigma$ for all $K$, meaning that the covariance 
matrix for each class is the same.

To classify a new observation, $X_0$, we find the category with the highest probability,
$P(y=k|X_0)$, for each $k=1,\ldots,K$. In order to do this, take the log, such that

\begin{align*}
  \log(P(y=k|X)) = \log(\pi_k) + \log(f_k(x)) - \log(\sum_{i=1}^K\pi_i f_i(x))
\end{align*}

When maximizing, the last term does not contribute with anything since it is 
constant, thus we can remove it. $\pi_k$ has already been covered, so we need
$\log(f_k(x))$. Because it was assumed to be multivariate normal with the same
covariance matrix between groups, we get 

\begin{align*}
\log(P(y=k|X)) = \log(\pi_k) - \log((2\pi)^{p/2}|\Sigma|^{1/2}) - \frac{1}{2}
(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)
\end{align*}

Then, the middle term does not contribute with anything because $\Sigma$ is 
the same for each group, so we can leave it. Hence

\begin{align*}
\log(P(y=k|X)) = \log(\pi_k) - \frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)
\end{align*}

Note that 

\begin{align*}
(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) &= (x^T-\mu_k^T)\Sigma^{-1}(x-\mu_k)\\
&= x^T\Sigma^{-1}x-x^T\Sigma^{-1}\mu_k-\mu_k^T\Sigma^{-1}x+\mu_k^T\Sigma^{-1}\mu_k
\end{align*}

Hence the first term does not depend on $k$ and is thus constant in each group,
once again by the assumption on $\Sigma$. Additionally, not that the second and 
third term are scalars, and they are in fact the transpose of each other, meaning
they are equal, i.e. $x^T\Sigma^{-1}\mu_k = \mu_k^T\Sigma^{-1}x$. With this in 
mind, we get

\begin{align*}
\log(P(y=k|X)) = \log(\pi_k) + x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}
\mu_k =: \delta_k(x)
\end{align*}

$\delta_k(x)$ is known as the linear discriminant functions. We classify a new observation
to class $i$ such that $\delta_i(x)$ is maximized. It is linear in $x$, hereby
the name. We thus needs to estimate $\mu_k, \Sigma, \pi_k$. They are estimated by

\begin{align*}
\hat{\pi}_k = \frac{n_k}{n}
\end{align*}

where $n_k$ is the number of observations in class $k$. 

\begin{align*}
\hat{\mu}_k = \frac{1}{n_k}\sum_{y_i = k}x_i
\end{align*}

which is the mean for class $k$.

\begin{align*}
  \hat{\Sigma} = \frac{1}{n-K}\sum_{k=1}^K\sum_{y_i=k}(x_i-\hat{\mu}_k)(x_i-\hat{\mu}_k)^T
\end{align*}

LDA tends to work better when dealing with factors with more than 2 levels 
compared to the one-vs-all classification. Because the structure in the explanatory
variables is linear, the classification regions by LDA are hyperplanes. Also, 
performs well when normality assumption is not true. The same goes for QDA. 

## Quadratic Discrimant Analysis (QDA)

It is really similar to LDA, the setup is the same, but this time we do not assume
that the covariance matrix in the different groups are the same. Thus with the 
framework above, we have

\begin{align*}
  f_k(x)=\frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2}(
x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)\right), \quad k\in\{1,\ldots,K\}
\end{align*}

Following the same procedure as before, the (quadratic) discriminant function 
has the form 

\begin{align*}
\delta_k(x) = \log(\pi_k) -\frac{1}{2}x^T\Sigma_k^{-1}x + x^T\Sigma_k^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma_k^{-1}\mu_k
\end{align*}

Hence the different covariance matrices has to be estimated. The discriminant 
function is now non-linear in $x$ due to the $x^T\Sigma_k^{-1}x$. The method
can be computationally heavy due to the many estimations in the case of many 
groups.  This time we estimate

\begin{align*}
  \Sigma_k = \frac{1}{n_k-1}\sum_{y_i=k}(x_i-\hat{\mu}_k)(x_i-\hat{\mu}_k)^T
\end{align*}

The quadratic form allows the boundaries between the regions to be of non-linear
form. Recall that this estimate is the unbiased estimator of $\Sigma_k$. More 
values improves flexibility, i.e. increasing variance but reduces bias. Classic
bias/variance trade-off. 

## (Naive) Bayes 

(Naive) Bayes Classifier is almost like the LDA and QDA. It uses Bayes' theorem
like previously;

\begin{align*}
P(y=k|X) = \frac{P(y=k)P(X|y=k)}{P(X)}
\end{align*}

Then the Bayes classifier disregards the denominator, because it is the same
for all classes. Hence Bayes Classifier assigns an object to the class that it is
most likely to be part of, that is

\begin{align*}
\hat{y} = \arg\ \underset{k}{\max}\ P(y=k)P(X|y=k)
\end{align*}

The (Naive) Bayes then as previously estimates $P(y=k)$ as the proportion of the
amount of objects in class $k$, just like in LDA and QDA. . Recall that $X$ is a $p-$dimensional vector, thus $P(X|y=k)$ comes from a multivariate distribution. 
Estimating this multivariate distribution is computationally heavy, which implies
that the method is unstable due to the curse of dimensionality. If we further assume 
that the components of $X$ are independent within each class, we get the Naive
Bayes classifier. This is much simpler to estimate. Hence by this assumption, we
can write

\begin{align*}
P(X|y=k) = \prod_{j=1}^PP(X_j|y = k)
\end{align*}

which implies that all the univariate marginal distribution can be estimated 
separately. Typical choice of the univarite distrubtions are Normal, so we need
to estimate 

\begin{align*}
X_j|(y=k) \sim N(\mu_{j,k},\sigma^2_{j,k})
\end{align*}

That is, compute the mean and variance for each univariate distribution. We once
again estimate 

\begin{align*}
\hat{\mu}_{j,k} = \frac{1}{N_k}\sum_{x_i|y_i = k}x_{j,k}
\end{align*}

and

\begin{align*}
\hat{\sigma}_{j,k}^2 = \frac{1}{N_k}\sum_{x_i|y_i = k}(x_{j,k}-\hat{\mu}_{j,k})^2
\end{align*}

### Considerations

Assumption of independence seems harsh. Tends to work anyway. Great for avoiding
the curse of dimensionality. Less observations needed to estimate univariate 
distributions than for a multivariate. If the assumption is not true, risk of bias.
Kind of like the bias/variance trade-off. Depending on data, other distributions
can be used, not only the normal distribution, e.g. $t$ distribution, log-normal.

## Practical application

In this part we are going to use LDA, QDA and Naive Bayes on the iris data set,
by only using the two first columns (Sepal Length and Sepal Width)

```{r}
suppressMessages(library(MASS))
suppressMessages(library(tidyverse))
suppressMessages(library(naivebayes))
data("iris") 
Data <- iris %>% dplyr::select(Sepal.Length, Sepal.Width, Species)
Data$Species <- as.numeric(Data$Species) #Selecting the Categories and the two first
#columns
head(Data) # A look at the data

#LDA fit

lda.fit <- lda(Species ~ ., data = Data)

#QDA fit

qda.fit <- qda(Species ~ ., data = Data)

#Naive Bayes

bayes_fit <- naive_bayes(as.factor(Species) ~ ., data = Data)

#Making a grid to present the results;

xmin <- min(Data$Sepal.Length)-1
xmax <- max(Data$Sepal.Length)+1
ymin <- min(Data$Sepal.Width)-1
ymax <- max(Data$Sepal.Width)+1

#Generating plots

Points <- 100

x_grid = seq(xmin, xmax, length.out = Points)
y_grid <- seq(ymin, ymax, length.out = Points)

par(mfrow=c(1,3))
par(mar= c(0.1,0.1,2,0.1))
#The plot
plot((Data %>% dplyr::select(-Species)), pch = Data$Species-1, 
     col = Data$Species+1, main = "LDA", 
     yaxt = "n", xaxt = "n")

#Predictions for LDA

for (j in 1:Points) {
  for (k in 1:Points) {
    new <- data.frame(Sepal.Length = x_grid[j], Sepal.Width = y_grid[k])
    new.pred <- predict(lda.fit, newdata = new)
    curr_obs <- as.numeric(new.pred$class)
    points(x_grid[j], y_grid[k], col = curr_obs+1, pch = 3, cex = 0.5)
  }
}
#Plot

plot((Data %>% dplyr::select(-Species)), pch = Data$Species-1, 
     col = Data$Species+1, main = "QDA", 
     yaxt = "n", xaxt = "n")

#Predictions for QDA

for (j in 1:Points) {
  for (k in 1:Points) {
    new <- data.frame(Sepal.Length = x_grid[j], Sepal.Width = y_grid[k])
    new.pred <- predict(qda.fit, newdata = new)
    curr_obs <- as.numeric(new.pred$class)
    points(x_grid[j], y_grid[k], col = curr_obs+1, pch = 3, cex = 0.5)
  }
}

#Plot

plot((Data %>% dplyr::select(-Species)), pch = Data$Species-1, 
     col = Data$Species+1, main = "Naive Bayes", 
     yaxt = "n", xaxt = "n")

for (j in 1:Points) {
  for (k in 1:Points) {
    new <- data.frame(Sepal.Length = x_grid[j], Sepal.Width = y_grid[k])
    new.pred <- predict(bayes_fit, newdata = new)
    curr_obs <- as.numeric(new.pred)
    points(x_grid[j], y_grid[k], col = curr_obs+1, pch = 3, cex = 0.5)
  }
}

```

